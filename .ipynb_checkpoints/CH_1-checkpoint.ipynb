{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### 1.1 Advantages of Longitudinal Studies:\n",
    "\n",
    "Longitudinal methods help because:\n",
    "\n",
    "* To achieve similar level of stat power, fewer subject are needed.\n",
    "* reapeated nature of study makes the diviations of individual subjects more significant.\n",
    "\n",
    "* each subject serves as his/her own control\n",
    "\n",
    "* Statistical estimates of individual trends can be used to better understand heterogeneity in the population and the determinants of growth and change at the level of the individual.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "#### 1. 2 CHALLENGES OF LONGITUDINAL DATA ANALYSIS:\n",
    "\n",
    "* Observations are not, by definition, independent and we must account for the dependency in data using more sophisticated statistical methods.\n",
    "\n",
    "\n",
    "\n",
    "* Often, there is a lack of available computer software for application of these more complex statistical models, or the level of statistical sophistication required of the user is beyond the typical level of the practitioner. In certain cases, for example nonlinear models for binary, ordinal, or nominal endpoints, parameter estimation can be computationally intensive due to the need for numerical or Monte Carlo simulation methods to evaluate the likelihood of nonlinear mixed-effects regression models.\n",
    "\n",
    "* more likely that there are missing data due to attrition\n",
    "* the values of the predictors or independent variables can also change over time\n",
    "\n",
    "* \"The treatment of time-varying covariates in analysis of longitudinal data permits much stronger statistical inferences about dynamical relationships than can be obtained using cross-sectional data. The price, however, is considerable added complexity to the statistical model.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A Logistic Regression can also be used to classify binary outcomes. The Logistic Regression model employs a sigmoid function to derive its classification and can be represented as a probability : \n",
    "\n",
    "$$P(y = C_1 | X) = \\sigma{(W^T\\cdot{x} + w_0)}$$\n",
    "\n",
    "where $\\sigma{(W^T\\cdot{x} + w_0)}$ is the inverse Logit function:\n",
    "\n",
    "$$\\sigma{(W^T\\cdot{x} + w_0)} = \\frac{1}{1 + e^{-(W^T\\cdot{x} + w_0)}}$$\n",
    "\n",
    "We can include the extra $w_0$ within the $W$ matrix to account for the bias, or intercept weight component. For the posterior probability of another class:\n",
    "\n",
    "$$P(y = C_2 | X) = 1-P(y = C_1 | X)$$\n",
    "\n",
    "To get the likelihood we can use the Binomial Distribution, however we would have to use it for only two values:\n",
    "\n",
    "$$P( Y | X) = \\prod_{n=1}^{N}  (W^T\\cdot{X})_{n}^{y_n}* (1 - (W^T\\cdot{X})_{n})^{1-y_n}$$\n",
    "\n",
    "To derive an error function for this likelihood function, a log transformation must be taken to change the product to a summation. From this a Gradient Decent optimization can be used to find the optimal weight vectors. To classify the Iris data set we need to build 3 individual classifiers using a One vs All approach.\n",
    "\n",
    "\n",
    "\n",
    "The **Direct Multiclass Logistic Regression** allows for a more Bayesian approach when calculating the posterior probabilities for classification. With this approach to Logistic Regression only one gradient descent optimization is needed. Instead of using the Inverse Logit Function, the Multiclass Logistic Regression uses a Softmax Transformation :\n",
    "\n",
    "$$P(y = C_k | X) = \\frac{e^(W_k^T\\cdot{x})}{\\sum_{j=1}^{K}e^(W_j^T\\cdot{x})}$$\n",
    "\n",
    "If we commit to transforming the 'species' column to dummy variables then a likelihood function can be derived as :\n",
    "\n",
    "$$ P(Y | w_1, ..., w_k) = \\prod_{n=1}^{N}\\prod_{k=1}^{K} P(y = C_k|X_n)^{y_{nk}} $$\n",
    "\n",
    "where $Y$ is a matrix with dimensions $N\\times K$ because of the dummy variable transformation. For gradient decent to work we need the Cross-Entropy Error Function. Appling a log to the Likelihood function allows for the removal of the products since summations are much easier for computers to work with :\n",
    "\n",
    "$$ -\\log(P(Y | w_1, ..., w_k))$$\n",
    "\n",
    "$$ = - \\sum_{n=1}^{N}\\sum_{k=1}^{K} y_nk\\log{(P(y = C_k | X))} $$ \n",
    "\n",
    "Next the gradient of the Log Likelihood function is needed to run the Gradient Descent:\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial{W_k}}{[- \\sum_{n=1}^{N}\\sum_{k=1}^{K} y_nk\\log{(P(y = C_k | X))}]}$$\n",
    "\n",
    "$$ = \\sum_{n=1}^{N}(P(y = C_k | X)_n - y_{nk})X_n$$\n",
    "\n",
    "Weights Update Function for this Stochastic Gradient Decent is:\n",
    "\n",
    "$$ W_{k, updated} = W_{k, prev} - \\eta \\frac{\\partial}{\\partial W_k}[\\sum_{n=1}^{N}(P(y = C_k | X)_n - y_{nk})X_n]$$\n",
    "Where $\\eta$ is the Learning Rate\n",
    "\n",
    "A far more detailed explanation can be found here: \n",
    "http://www.cedar.buffalo.edu/~srihari/CSE574/Chap4/4.3.4-MultiLogistic.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
